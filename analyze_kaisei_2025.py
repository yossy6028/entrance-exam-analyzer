#!/usr/bin/env python3
"""
é–‹æˆä¸­å­¦æ ¡2025å¹´åº¦å…¥è©¦å•é¡Œåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Google Cloud Vision APIã‚’ä½¿ç”¨ã—ã¦PDFã‹ã‚‰ç›´æ¥åˆ†æ
"""
import re
from pathlib import Path
import pandas as pd
from datetime import datetime
from modules.pdf_processor import PDFProcessor
from modules.ocr_handler import OCRHandler
from modules.text_analyzer import TextAnalyzer
from modules.pattern_extractor import PatternExtractor
from modules.excel_writer import ExcelWriter
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/analysis.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def analyze_kaisei_2025(pdf_path: str):
    """é–‹æˆä¸­2025å¹´åº¦ã®å…¥è©¦å•é¡Œã‚’åˆ†æ"""
    
    print(f"\n{'='*60}")
    print(f"é–‹æˆä¸­å­¦æ ¡2025å¹´åº¦ å…¥è©¦å•é¡Œåˆ†æ")
    print(f"{'='*60}")
    
    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not Path(pdf_path).exists():
        print(f"âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_path}")
        return None
    
    print(f"\nğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«: {Path(pdf_path).name}")
    
    # 1. PDFå‡¦ç†
    print(f"\nã€1. PDFå‡¦ç†ã€‘")
    pdf_processor = PDFProcessor()
    images = pdf_processor.convert_pdf_to_images(pdf_path)
    print(f"âœ… {len(images)}ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›")
    
    # 2. OCRå‡¦ç†
    print(f"\nã€2. OCRå‡¦ç†ã€‘")
    ocr_handler = OCRHandler()
    
    all_text = []
    for i, image in enumerate(images):
        print(f"  ãƒšãƒ¼ã‚¸{i+1}/{len(images)}ã‚’å‡¦ç†ä¸­...", end='', flush=True)
        result = ocr_handler.extract_text_from_image(image)
        text = result.get('full_text', '')
        all_text.append(text)
        print(f" âœ… ({len(text)}æ–‡å­—)")
    
    # ãƒ†ã‚­ã‚¹ãƒˆçµåˆ
    combined_text = "\n".join(all_text)
    print(f"\nğŸ“ æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: {len(combined_text)}æ–‡å­—")
    
    # OCRçµæœãŒå°‘ãªã™ãã‚‹å ´åˆã®è­¦å‘Š
    if len(combined_text) < 1000:
        print(f"\u26a0ï¸  è­¦å‘Š: OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒéå¸¸ã«å°‘ãªã„ã§ã™")
        print(f"\u62bdå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®å…ˆé ­100æ–‡å­—:")
        print(combined_text[:100])
        print("\nâ€» Google Cloud Vision APIã®èªè¨¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
    
    # 3. ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
    print(f"\nã€3. ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã€‘")
    
    # è¨­å•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®šç¾©ï¼ˆé–‹æˆä¸­å­¦æ ¡ç”¨ï¼‰
    question_patterns = {
        'æ¼¢å­—ãƒ»èªå¥': [
            r'æ¼¢å­—.*æ›¸ã.*ãªã•ã„',
            r'ã‚«ã‚¿ã‚«ãƒŠã‚’æ¼¢å­—ã«.*ãªã•ã„',
            r'ã²ã‚‰ãŒãªã‚’æ¼¢å­—ã«.*ãªã•ã„',
            r'æ„å‘³.*ç­”ãˆãªã•ã„',
            r'èªå¥.*èª¬æ˜.*ãªã•ã„'
        ],
        'é¸æŠ': [
            r'é¸ã³.*ãªã•ã„',
            r'è¨˜å·ã§ç­”ãˆãªã•ã„',
            r'æœ€ã‚‚.*ã‚‚ã®ã‚’.*é¸ã³',
            r'ã‚¢ï½[ã‚ªã‚«]ã‹ã‚‰.*é¸ã³',
            r'æ¬¡ã®ä¸­ã‹ã‚‰.*é¸ã³'
        ],
        'æŠœãå‡ºã—': [
            r'æŠœãå‡º.*ãªã•ã„',
            r'æ–‡ä¸­ã‹ã‚‰.*æŠœãå‡º',
            r'æœ¬æ–‡ä¸­ã®.*ã‚’ç­”ãˆãªã•ã„',
            r'æ›¸ãæŠœ.*ãªã•ã„'
        ],
        'è¨˜è¿°': [
            r'èª¬æ˜.*ãªã•ã„',
            r'ç†ç”±.*ç­”ãˆãªã•ã„',
            r'ã©ã®ã‚ˆã†ãª.*ã‹.*ç­”ãˆãªã•ã„',
            r'ãªãœ.*ã‹.*ç­”ãˆãªã•ã„',
            r'å¿ƒæƒ….*èª¬æ˜.*ãªã•ã„'
        ]
    }
    
    analyzer = TextAnalyzer(question_patterns)
    analysis_result = analyzer.analyze_exam_structure(combined_text)
    
    print(f"âœ… å¤§å•æ•°: {len(analysis_result['sections'])}")
    print(f"âœ… ç·è¨­å•æ•°: {len(analysis_result['questions'])}")
    
    # 4. å‡ºå…¸æƒ…å ±ã®æŠ½å‡º
    print(f"\nã€4. å‡ºå…¸æƒ…å ±ã®æŠ½å‡ºã€‘")
    
    # å‡ºå…¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®šç¾©
    source_patterns = [
        r'ï¼ˆ([^ï¼‰]+)ã€([^ã€]+)ã€[^ï¼‰]*ï¼‰',  # æ—¥æœ¬èªæ‹¬å¼§
        r'ï¼ˆ([^ï¼‰]+)ã€Œ([^ã€]+)ã€[^ï¼‰]*ï¼‰',  # æ—¥æœ¬èªæ‹¬å¼§ã€å¼•ç”¨ç¬¦
        r'\(([^)]+)ã€([^ã€]+)ã€[^)]*\)',  # åŠè§’æ‹¬å¼§
    ]
    
    pattern_extractor = PatternExtractor(source_patterns)
    
    sources = []
    for section in analysis_result['sections']:
        source_info = pattern_extractor.extract_source_info(section['text'])
        if source_info['author'] or source_info['title']:
            sources.append({
                'section': section['number'],
                **source_info
            })
            print(f"  å¤§å•{section['number']}: {source_info['author']} - {source_info['title'] or '(ä½œå“åä¸æ˜)'}")
    
    # 5. Excelå‡ºåŠ›
    print(f"\nã€5. çµæœã®ä¿å­˜ã€‘")
    
    # Excelç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    def sanitize_for_excel(text):
        """Excelä¿å­˜ç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""
        if not text:
            return ""
        
        # å•é¡Œã®ã‚ã‚‹ç‰¹æ®Šæ–‡å­—ã‚’å®šç¾©
        problematic_chars = {
            '\ufff9',  # Interlinear Annotation Anchor
            '\ufffa',  # Interlinear Annotation Separator  
            '\ufffb',  # Interlinear Annotation Terminator
            '\ufeff',  # Byte Order Mark
            '\u200b',  # Zero Width Space
            '\u200c',  # Zero Width Non-Joiner
            '\u200d',  # Zero Width Joiner
        }
        
        # ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
        cleaned = ''.join(char for char in text if char not in problematic_chars)
        
        # å±±æ‹¬å¼§ã€ˆã€‰ã‚’é€šå¸¸ã®æ‹¬å¼§ã«ç½®æ›
        cleaned = cleaned.replace('ã€ˆ', 'ï¼œ').replace('ã€‰', 'ï¼')
        
        # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»ï¼ˆæ”¹è¡Œãƒ»ã‚¿ãƒ–ã¯ä¿æŒï¼‰
        sanitized = ''.join(char for char in cleaned if ord(char) >= 32 or char in '\n\r\t')
        
        # Excelã§å•é¡Œã‚’èµ·ã“ã™å¯èƒ½æ€§ã®ã‚ã‚‹æ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£
        if sanitized.startswith('ï¼œ') or sanitized.startswith('ã€ˆ'):
            sanitized = 'ã€Œ' + sanitized[1:]
        if sanitized.endswith('ï¼') or sanitized.endswith('ã€‰'):
            sanitized = sanitized[:-1] + 'ã€'
        
        return sanitized[:500]  # é•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å½¢å¼ã§ä¿å­˜
    create_database_excel(analysis_result, sources, len(combined_text))
    
    # é€šå¸¸ã®åˆ†æçµæœã‚‚ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f"é–‹æˆä¸­å­¦æ ¡_2025_åˆ†æçµæœ_{timestamp}.xlsx"
    
    excel_writer = ExcelWriter('data/output')
    excel_writer.write_analysis_results(
        output_file,
        analysis_result,
        sources,
        {
            'school_name': 'é–‹æˆä¸­å­¦æ ¡',
            'year': '2025',
            'total_chars': len(combined_text)
        }
    )
    
    print(f"âœ… åˆ†æçµæœã‚’ä¿å­˜: {output_file}")
    
    return analysis_result


def create_database_excel(analysis_result, sources, total_chars):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å½¢å¼ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆãƒ»æ›´æ–°"""
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
    db_filename = "entrance_exam_database.xlsx"
    school_name = "é–‹æˆä¸­å­¦æ ¡"
    year = 2025
    
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    try:
        existing_sheets = pd.ExcelFile(db_filename, engine='openpyxl').sheet_names
    except FileNotFoundError:
        existing_sheets = []
    
    # Excelç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆé–¢æ•°å®šç¾©æ¸ˆã¿ï¼‰
    def sanitize_for_excel(text):
        if not text:
            return ""
        problematic_chars = {'\ufff9', '\ufffa', '\ufffb', '\ufeff', '\u200b', '\u200c', '\u200d'}
        cleaned = ''.join(char for char in text if char not in problematic_chars)
        cleaned = cleaned.replace('ã€ˆ', 'ï¼œ').replace('ã€‰', 'ï¼')
        sanitized = ''.join(char for char in cleaned if ord(char) >= 32 or char in '\n\r\t')
        if sanitized.startswith('ï¼œ') or sanitized.startswith('ã€ˆ'):
            sanitized = 'ã€Œ' + sanitized[1:]
        if sanitized.endswith('ï¼') or sanitized.endswith('ã€‰'):
            sanitized = sanitized[:-1] + 'ã€'
        return sanitized[:500]
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    data_row = {
        'å¹´åº¦': year,
        'ç·è¨­å•æ•°': len(analysis_result['questions']),
        'ç·æ–‡å­—æ•°': total_chars,
        'å¤§å•æ•°': len(analysis_result['sections'])
    }
    
    # å„å¤§å•ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    for i, section in enumerate(analysis_result['sections'], 1):
        # è©²å½“ã™ã‚‹å‡ºå…¸æƒ…å ±ã‚’æ¢ã™
        section_source = next((s for s in sources if s['section'] == section['number']), {})
        
        # æ–‡ç« ã‚¸ãƒ£ãƒ³ãƒ«ã¨ãƒ†ãƒ¼ãƒã‚’åˆ¤å®š
        section_text = section['text'][:1000]  # æœ€åˆã®1000æ–‡å­—ã§åˆ¤å®š
        
        # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®š
        if any(word in section_text for word in ['å°èª¬', 'ç‰©èª', 'ã€Œ', 'ã€', 'ã¨è¨€ã£ãŸ', 'ã¨æ€ã£ãŸ']):
            genre = 'å°èª¬ãƒ»ç‰©èª'
        elif any(word in section_text for word in ['è©•è«–', 'è«–èª¬', 'ã«ã¤ã„ã¦', 'ã¨ã„ã†', 'ã“ã¨ã¯']):
            genre = 'è©•è«–ãƒ»è«–èª¬'
        elif any(word in section_text for word in ['éšç­†', 'ã‚¨ãƒƒã‚»ã‚¤', 'ç§ã¯', 'çµŒé¨“']):
            genre = 'éšç­†ãƒ»ã‚¨ãƒƒã‚»ã‚¤'
        else:
            genre = 'è©•è«–ãƒ»è«–èª¬'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # ãƒ†ãƒ¼ãƒåˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if any(word in section_text for word in ['è‡ªç„¶', 'ç’°å¢ƒ', 'ç”Ÿç‰©', 'å‹•ç‰©']):
            theme = 'è‡ªç„¶ãƒ»ç’°å¢ƒ'
        elif any(word in section_text for word in ['ç¤¾ä¼š', 'äººé–“', 'æ–‡åŒ–', 'æ­´å²']):
            theme = 'ç¤¾ä¼šãƒ»æ–‡åŒ–'
        elif any(word in section_text for word in ['ç§‘å­¦', 'æŠ€è¡“', 'AI', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿']):
            theme = 'ç§‘å­¦ãƒ»æŠ€è¡“'
        elif any(word in section_text for word in ['å‹æƒ…', 'å®¶æ—', 'æˆé•·']):
            theme = 'äººé–“é–¢ä¿‚ãƒ»æˆé•·'
        else:
            theme = 'ä¸€èˆ¬'
        
        data_row[f'å¤§å•{i}_ã‚¸ãƒ£ãƒ³ãƒ«'] = genre
        data_row[f'å¤§å•{i}_ãƒ†ãƒ¼ãƒ'] = theme
        data_row[f'å¤§å•{i}_è‘—è€…'] = sanitize_for_excel(section_source.get('author', 'ä¸æ˜'))
        data_row[f'å¤§å•{i}_ä½œå“'] = sanitize_for_excel(section_source.get('title', 'ä¸æ˜'))
        data_row[f'å¤§å•{i}_è¨­å•æ•°'] = len([q for q in analysis_result['questions'] if q['section'] == section['number']])
        data_row[f'å¤§å•{i}_æ–‡å­—æ•°'] = len(section['text'])
    
    # è¨­å•ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
    for q_type, count in analysis_result['question_types'].items():
        data_row[f'{q_type}_å•é¡Œæ•°'] = count
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    new_df = pd.DataFrame([data_row])
    
    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    with pd.ExcelWriter(db_filename, engine='openpyxl', mode='a' if existing_sheets else 'w', if_sheet_exists='replace') as writer:
        if school_name in existing_sheets:
            # æ—¢å­˜ã‚·ãƒ¼ãƒˆã«è¿½åŠ 
            existing_df = pd.read_excel(db_filename, sheet_name=school_name)
            existing_df['å¹´åº¦'] = pd.to_numeric(existing_df['å¹´åº¦'], errors='coerce')
            if year in existing_df['å¹´åº¦'].values:
                existing_df = existing_df[existing_df['å¹´åº¦'] != year]
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            combined_df['å¹´åº¦'] = pd.to_numeric(combined_df['å¹´åº¦'], errors='coerce')
            combined_df = combined_df.sort_values('å¹´åº¦')
        else:
            combined_df = new_df
        
        combined_df.to_excel(writer, sheet_name=school_name, index=False)
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°: {db_filename} - {school_name}ã‚·ãƒ¼ãƒˆ")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # é–‹æˆä¸­2025å¹´ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    pdf_path = "/Users/yoshiikatsuhiko/Desktop/01_ä»•äº‹ (Work)/ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®¶åº­æ•™å¸«è³‡æ–™/éå»å•/é–‹æˆä¸­å­¦æ ¡/2025å¹´é–‹æˆä¸­å­¦æ ¡å•é¡Œ_å›½èª.pdf"
    
    # åˆ†æå®Ÿè¡Œ
    result = analyze_kaisei_2025(pdf_path)
    
    if result:
        print(f"\nâœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"\nã€åˆ†æã‚µãƒãƒªãƒ¼ã€‘")
        print(f"- å¤§å•æ•°: {len(result['sections'])}")
        print(f"- ç·è¨­å•æ•°: {len(result['questions'])}")
        print(f"- è¨­å•ã‚¿ã‚¤ãƒ—:")
        for q_type, count in result['question_types'].items():
            percentage = (count / len(result['questions'])) * 100
            print(f"  - {q_type}: {count}å• ({percentage:.1f}%)")


if __name__ == "__main__":
    main()