#!/usr/bin/env python3
"""
æ¸‹æ¸‹ä¸­å­¦æ ¡2015å¹´åº¦å…¥è©¦å•é¡Œåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
bunkoOCRã§å‡¦ç†ã—ãŸé«˜ç²¾åº¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
"""
import re
from pathlib import Path
import pandas as pd
from datetime import datetime


def analyze_shibuya_2015_from_bunko_text(text_file_path: str):
    """bunkoOCRã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¸‹æ¸‹15å¹´åº¦ã‚’åˆ†æ"""
    
    print(f"\n{'='*60}")
    print(f"æ¸‹æ¸‹ä¸­å­¦æ ¡2015å¹´åº¦ å…¥è©¦å•é¡Œåˆ†æ")
    print(f"{'='*60}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(text_file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    print(f"\nğŸ“„ èª­ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆ: {len(text)}æ–‡å­—")
    
    # å¤§å•æ§‹é€ ã®æ¤œå‡º
    sections = []
    
    # å¤§å•ä¸€ã®æ¤œå‡ºï¼ˆbunkoOCRã®çµæœã‹ã‚‰ã€å¤§å•ã®é–‹å§‹ã‚’æ¤œå‡ºï¼‰
    # ã€Œæ¬¡ã®æ–‡ç« ã‚’èª­ã‚“ã§ã€ã¨ã„ã†ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
    section1_match = re.search(r'æ¬¡ã®æ–‡ç« ã‚’èª­ã‚“ã§[å¾Œ|ã‚ã¨]?ã®?å•ã„ã«ç­”ãˆãªã•ã„', text)
    if section1_match:
        sections.append({
            'section_num': 1,
            'type': 'æ–‡ç« èª­è§£',
            'start_pos': section1_match.start()
        })
        print(f"\nâœ… å¤§å•ä¸€ã‚’æ¤œå‡º: æ–‡ç« èª­è§£")
    
    # å¤§å•äºŒã®æ¤œå‡ºï¼ˆ2ã¤ç›®ã®ã€Œæ¬¡ã®æ–‡ç« ã‚’èª­ã‚“ã§ã€ã‚’æ¢ã™ï¼‰
    if section1_match:
        section2_match = re.search(r'æ¬¡ã®æ–‡ç« ã‚’èª­ã‚“ã§[å¾Œ|ã‚ã¨]?ã®?å•ã„ã«ç­”ãˆãªã•ã„', text[section1_match.end():])
        if section2_match:
            sections.append({
                'section_num': 2,
                'type': 'æ–‡ç« èª­è§£',
                'start_pos': section1_match.end() + section2_match.start()
            })
            print(f"âœ… å¤§å•äºŒã‚’æ¤œå‡º: æ–‡ç« èª­è§£")
    
    # å¤§å•ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€å…¨ä½“ã‚’1ã¤ã®å¤§å•ã¨ã—ã¦æ‰±ã†
    if not sections:
        sections.append({
            'section_num': 1,
            'type': 'æ–‡ç« èª­è§£',
            'start_pos': 0
        })
        print(f"\nâœ… å…¨ä½“ã‚’å¤§å•ä¸€ã¨ã—ã¦å‡¦ç†")
    
    print(f"\næ¤œå‡ºã•ã‚ŒãŸå¤§å•æ•°: {len(sections)}")
    
    # è¨­å•ã®æ¤œå‡º
    all_questions = []
    
    # å„å¤§å•å†…ã®è¨­å•ã‚’æ¤œå‡º
    for i, section in enumerate(sections):
        section_num = section['section_num']
        
        # å¤§å•ã®ç¯„å›²ã‚’ç‰¹å®š
        if i < len(sections) - 1:
            section_text = text[section['start_pos']:sections[i+1]['start_pos']]
        else:
            section_text = text[section['start_pos']:]
        
        print(f"\nã€å¤§å•{section_num}ã®è¨­å•æ¤œå‡ºã€‘")
        
        # bunkoOCRã®çµæœã‹ã‚‰è¨­å•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šç¾©
        question_patterns = [
            # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå•ä¸€ã€å•äºŒãªã©ï¼‰
            (r'å•([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)', 'kanji_num'),
            # OCRèª¤èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé–“ä¸‰ãªã©ï¼‰
            (r'é–“([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)', 'kanji_num_ocr'),
            # å•1ã€å•2ãªã©ã®æ•°å­—ãƒ‘ã‚¿ãƒ¼ãƒ³
            (r'å•([1-9])', 'hankaku_num'),
        ]
    
    # ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰è¨­å•ã‚’æ¤œå‡º
    print(f"\nã€è¨­å•æ¤œå‡ºã€‘")
    
    for pattern, pattern_type in question_patterns:
        for match in re.finditer(pattern, text):
            question_num = match.group(1) if match.groups() else None
            
            # è¨­å•ã®å‘¨è¾ºãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆå‰å¾Œ200æ–‡å­—ï¼‰
            start = max(0, match.start() - 100)
            end = min(len(text), match.end() + 200)
            context = text[start:end].replace('\n', ' ')
            
            # è¨­å•ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            if 'æ¼¢å­—' in context or 'ã‚«ã‚¿ã‚«ãƒŠã‚’æ¼¢å­—ã«' in context:
                q_type = 'æ¼¢å­—ãƒ»èªå¥'
            elif 'è¨˜å·ã§ç­”ãˆãªã•ã„' in context:
                q_type = 'é¸æŠ'
            elif 'èª¬æ˜ã—ãªã•ã„' in context or 'ç†ç”±' in context:
                q_type = 'è¨˜è¿°'
            elif 'æŠœãå‡º' in context:
                q_type = 'æŠœãå‡ºã—'
            else:
                q_type = 'è¨˜è¿°'
            
            # å¤§å•ã‚’æ¨å®šï¼ˆæ–‡æ›¸ã®å‰åŠã‹å¾ŒåŠã‹ï¼‰
            if match.start() < len(text) // 2:
                section_num = 1
            else:
                section_num = 2
            
            question_info = {
                'section': section_num,
                'question': match.group(0),
                'type': q_type,
                'position': match.start(),
                'context': context[:150] + '...' if len(context) > 150 else context
            }
            
            # é‡è¤‡ã‚’é¿ã‘ã‚‹ï¼ˆä½ç½®ãŒè¿‘ã„ã‹åŒã˜è¨­å•åãªã‚‰é‡è¤‡ã¨ã¿ãªã™ï¼‰
            is_duplicate = False
            for existing_q in all_questions:
                # ä½ç½®ãŒè¿‘ã„å ´åˆ
                if abs(existing_q['position'] - question_info['position']) < 50:
                    is_duplicate = True
                    break
                # åŒã˜å¤§å•å†…ã§åŒã˜è¨­å•åã®å ´åˆ
                if existing_q['section'] == section_num and existing_q['question'] == match.group(0):
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                all_questions.append(question_info)
                print(f"  {match.group(0)} - {q_type} (å¤§å•{section_num})")
    
    # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
    all_questions.sort(key=lambda x: x['position'])
    
    print(f"\nğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
    print(f"ç·è¨­å•æ•°: {len(all_questions)}")
    
    # è¨­å•ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
    type_counts = {}
    for q in all_questions:
        q_type = q['type']
        type_counts[q_type] = type_counts.get(q_type, 0) + 1
    
    print(f"\nè¨­å•ã‚¿ã‚¤ãƒ—åˆ¥å†…è¨³:")
    for q_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(all_questions)) * 100
        print(f"  {q_type}: {count}å• ({percentage:.1f}%)")
    
    # å‡ºå…¸æƒ…å ±ã®æŠ½å‡º
    print(f"\nğŸ“š å‡ºå…¸æƒ…å ±ã®æ¤œç´¢")
    
    # å‡ºå…¸ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ–‡æœ«ã®æ‹¬å¼§å†…ï¼‰- ã‚ˆã‚Šå³å¯†ã«çŸ­ã„æ–‡å­—åˆ—ã®ã¿
    source_patterns = [
        r'ï¼ˆ([^ï¼‰]{1,50}ã€[^ã€]{1,100}ã€[^ï¼‰]{0,20})ï¼‰',  # æ—¥æœ¬èªæ‹¬å¼§ã€é•·ã•åˆ¶é™
        r'ï¼ˆ([^ï¼‰]{1,50}ã€Œ[^ã€]{1,100}ã€[^ï¼‰]{0,20})ï¼‰',   # æ—¥æœ¬èªæ‹¬å¼§ã€å¼•ç”¨ç¬¦
        r'\(([^)]{1,50}ã€[^ã€]{1,100}ã€[^)]{0,20})\)',  # åŠè§’æ‹¬å¼§
    ]
    
    sources = []
    for pattern in source_patterns:
        for match in re.finditer(pattern, text):
            source_text = match.group(1)
            # é•·ã™ãã‚‹ãƒãƒƒãƒã¯é™¤å¤–ï¼ˆOCRã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ï¼‰
            if len(source_text) > 150:
                continue
            # è‘—è€…åã¨ä½œå“åã‚’æŠ½å‡º
            if 'ã€' in source_text:
                parts = source_text.split('ã€')
                if len(parts) >= 2:
                    author = parts[0].strip()
                    title = parts[1].split('ã€')[0].strip()
                    # è‘—è€…åãŒçŸ­ã™ãã‚‹ã‹é•·ã™ãã‚‹å ´åˆã¯é™¤å¤–
                    if 1 <= len(author) <= 20 and 1 <= len(title) <= 50:
                        sources.append({
                            'author': author,
                            'title': title,
                            'full': source_text,
                            'position': match.start()
                        })
    
    # å„å¤§å•ã®å‡ºå…¸ã‚’ç‰¹å®šã—ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã«è¿½åŠ 
    for section in sections:
        section_sources = [s for s in sources if s['position'] > section['start_pos']]
        if section_sources:
            # æœ€ã‚‚è¿‘ã„å‡ºå…¸ã‚’é¸æŠ
            closest_source = min(section_sources, key=lambda x: x['position'] - section['start_pos'])
            section['author'] = closest_source['author']
            section['title'] = closest_source['title']
            print(f"\nå¤§å•{section['section_num']}ã®å‡ºå…¸:")
            print(f"  è‘—è€…: {closest_source['author']}")
            print(f"  ä½œå“: ã€{closest_source['title']}ã€")
        else:
            section['author'] = 'ä¸æ˜'
            section['title'] = 'ä¸æ˜'
    
    # Excelç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    def sanitize_for_excel(text):
        """Excelä¿å­˜ç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""
        if not text:
            return ""
        
        # å•é¡Œã®ã‚ã‚‹ç‰¹æ®Šæ–‡å­—ã‚’å®šç¾©
        # U+FFF9-U+FFFB: Interlinear Annotation Characters (ãƒ«ãƒ“ç”¨æ–‡å­—)
        # U+FEFF: Byte Order Mark
        # U+200B-U+200D: Zero Width Spaceé¡
        problematic_chars = {
            '\ufff9',  # Interlinear Annotation Anchor
            '\ufffa',  # Interlinear Annotation Separator  
            '\ufffb',  # Interlinear Annotation Terminator
            '\ufeff',  # Byte Order Mark
            '\u200b',  # Zero Width Space
            '\u200c',  # Zero Width Non-Joiner
            '\u200d',  # Zero Width Joiner
        }
        
        # ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
        cleaned = ''.join(char for char in text if char not in problematic_chars)
        
        # å±±æ‹¬å¼§ã€ˆã€‰ã‚’é€šå¸¸ã®æ‹¬å¼§ã«ç½®æ›
        cleaned = cleaned.replace('ã€ˆ', 'ï¼œ').replace('ã€‰', 'ï¼')
        
        # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»ï¼ˆæ”¹è¡Œãƒ»ã‚¿ãƒ–ã¯ä¿æŒï¼‰
        sanitized = ''.join(char for char in cleaned if ord(char) >= 32 or char in '\n\r\t')
        
        # Excelã§å•é¡Œã‚’èµ·ã“ã™å¯èƒ½æ€§ã®ã‚ã‚‹æ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£
        # å…ˆé ­ã®ç‰¹æ®Šæ‹¬å¼§ãŒåŸå› ã§è¡¨ç¤ºãŒåˆ‡ã‚Œã‚‹å•é¡Œã‚’å›é¿
        if sanitized.startswith('ï¼œ') or sanitized.startswith('ã€ˆ'):
            sanitized = 'ã€Œ' + sanitized[1:]
        if sanitized.endswith('ï¼') or sanitized.endswith('ã€‰'):
            sanitized = sanitized[:-1] + 'ã€'
        
        return sanitized[:500]  # é•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
    
    # Excelå½¢å¼ã§çµæœã‚’ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f"æ¸‹æ¸‹ä¸­å­¦æ ¡_2015_bunkoåˆ†æçµæœ_{timestamp}.xlsx"
    
    # æ–‡ç« ã‚¸ãƒ£ãƒ³ãƒ«ãƒ»ãƒ†ãƒ¼ãƒåˆ¤å®šé–¢æ•°
    def determine_genre_and_theme(text_content, author, title):
        """æ–‡ç« ã®ã‚¸ãƒ£ãƒ³ãƒ«ã¨ãƒ†ãƒ¼ãƒã‚’åˆ¤å®š"""
        
        # å°èª¬ãƒ»æ–‡å­¦çš„è¦ç´ ã®æ¤œå‡º
        fiction_indicators = [
            'ã ã£ãŸ', 'ã§ã‚ã‚‹', 'ã¨æ€ã£ãŸ', 'ã®ã ã£ãŸ', 'ã§ã‚ã£ãŸ',
            'ã€Œ', 'ã€', 'ã¨è¨€ã£ãŸ', 'ã¨ç­”ãˆãŸ', 'ã¤ã¶ã‚„ã„ãŸ', 'ã¨å«ã‚“ã ',
            'ã®å¿ƒ', 'ã®æ°—æŒã¡', 'æ„Ÿã˜ãŸ', 'æ€ã„å‡ºã—ãŸ', 'æƒ³åƒã—ãŸ',
            'ã‚ã‚†ã¿', 'åœ’è‘‰', 'ã¡ã‚ƒã‚“', 'ãŠæ¯ã•ã‚“', 'ãƒ©ãƒ³ãƒ‰ã‚»ãƒ«'
        ]
        
        # è©•è«–ãƒ»è«–èª¬çš„è¦ç´ ã®æ¤œå‡º
        essay_indicators = [
            'ã§ã‚ã‚‹', 'ã§ã¯ãªã„', 'ã«ã¤ã„ã¦', 'ã«é–¢ã—ã¦', 'ã«ãŠã„ã¦',
            'è€ƒãˆã‚‹', 'æ€è€ƒ', 'åˆ†æ', 'æ¤œè¨', 'è­°è«–', 'ä¸»å¼µ',
            'ã¨ã„ã†æ¦‚å¿µ', 'ã¨ã¯ä½•ã‹', 'ã®æ„å‘³', 'ã®å®šç¾©',
            'å…¬æ­£', 'ãƒ•ã‚§ã‚¢ãƒã‚¹', 'æ­£ç¾©', 'ç¤¾ä¼š', 'å“²å­¦'
        ]
        
        # éšç­†ãƒ»ã‚¨ãƒƒã‚»ã‚¤çš„è¦ç´ ã®æ¤œå‡º
        essay_personal_indicators = [
            'ã§ã‚ã‚‹', 'ã¨æ€ã†', 'ã®ã§ã‚ã‚‹', 'ã ã‚ã†', 'ã‹ã‚‚ã—ã‚Œãªã„',
            'ç§ã¯', 'ç­†è€…ã¯', 'è‘—è€…ã¯', 'çµŒé¨“', 'ä½“é¨“', 'æ„Ÿæƒ³'
        ]
        
        fiction_score = sum(1 for indicator in fiction_indicators if indicator in text_content)
        essay_score = sum(1 for indicator in essay_indicators if indicator in text_content)
        personal_essay_score = sum(1 for indicator in essay_personal_indicators if indicator in text_content)
        
        # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®š
        max_score = max(fiction_score, essay_score, personal_essay_score)
        
        if max_score == 0:
            genre = "æ–‡ç« èª­è§£"
        elif fiction_score == max_score:
            genre = "å°èª¬ãƒ»ç‰©èª"
        elif essay_score == max_score:
            genre = "è©•è«–ãƒ»è«–èª¬"
        else:
            genre = "éšç­†ãƒ»ã‚¨ãƒƒã‚»ã‚¤"
        
        # ãƒ†ãƒ¼ãƒãƒ»ä¸»é¡Œã®åˆ¤å®š
        theme = "ä¸€èˆ¬"
        
        # å…·ä½“çš„ãªãƒ†ãƒ¼ãƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§åˆ¤å®š
        if any(keyword in text_content for keyword in ['å…¬æ­£', 'ãƒ•ã‚§ã‚¢ãƒã‚¹', 'æ­£ç¾©', 'å¹³ç­‰']):
            theme = "ç¤¾ä¼šãƒ»æ­£ç¾©"
        elif any(keyword in text_content for keyword in ['å‹æƒ…', 'å‹é”', 'ä»²é–“', 'ãŠå°é£ã„', 'å®¶æ—']):
            theme = "äººé–“é–¢ä¿‚ãƒ»æˆé•·"
        elif any(keyword in text_content for keyword in ['æ•™è‚²', 'å­¦æ ¡', 'å‹‰å¼·', 'å—é¨“']):
            theme = "æ•™è‚²ãƒ»å­¦ç¿’"
        elif any(keyword in text_content for keyword in ['è‡ªç„¶', 'ç’°å¢ƒ', 'å‹•ç‰©', 'æ¤ç‰©']):
            theme = "è‡ªç„¶ãƒ»ç’°å¢ƒ"
        elif any(keyword in text_content for keyword in ['æ­´å²', 'æ–‡åŒ–', 'ä¼çµ±', 'ç¤¾ä¼š']):
            theme = "æ­´å²ãƒ»æ–‡åŒ–"
        elif any(keyword in text_content for keyword in ['ç§‘å­¦', 'æŠ€è¡“', 'ç ”ç©¶', 'ç™ºè¦‹']):
            theme = "ç§‘å­¦ãƒ»æŠ€è¡“"
        elif any(keyword in text_content for keyword in ['èŠ¸è¡“', 'éŸ³æ¥½', 'çµµç”»', 'æ–‡å­¦']):
            theme = "èŠ¸è¡“ãƒ»æ–‡åŒ–"
        elif any(keyword in text_content for keyword in ['å¿ƒ', 'æ„Ÿæƒ…', 'æ€æƒ³', 'å“²å­¦']):
            theme = "å¿ƒç†ãƒ»å“²å­¦"
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã®ãƒ’ãƒ³ãƒˆ
        if title:
            if 'å…¬æ­£' in title or 'ãƒ•ã‚§ã‚¢ãƒã‚¹' in title or 'æ­£ç¾©' in title:
                theme = "ç¤¾ä¼šãƒ»æ­£ç¾©"
            elif 'ãŠå°é£ã„' in title or 'æˆæœ' in title:
                theme = "äººé–“é–¢ä¿‚ãƒ»æˆé•·"
        
        return genre, theme

    # å¤§å•åˆ¥ã®çµ±è¨ˆã‚’è¨ˆç®—
    section_stats = []
    for section in sections:
        section_num = section['section_num']
        section_questions = [q for q in all_questions if q['section'] == section_num]
        
        # å¤§å•ã®æ–‡å­—æ•°ã‚’æ¨å®šï¼ˆã“ã®å¤§å•ã®ç¯„å›²ã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        if len(sections) > section_num - 1:
            if section_num < len(sections):
                next_section_start = sections[section_num]['start_pos'] if section_num < len(sections) else len(text)
                section_text_length = next_section_start - section['start_pos']
                section_text = text[section['start_pos']:next_section_start]
            else:
                section_text_length = len(text) - section['start_pos']
                section_text = text[section['start_pos']:]
        else:
            section_text_length = len(text) - section['start_pos']
            section_text = text[section['start_pos']:]
        
        # æ–‡ç« ã‚¸ãƒ£ãƒ³ãƒ«ã¨ãƒ†ãƒ¼ãƒã‚’åˆ¤å®š
        genre, theme = determine_genre_and_theme(
            section_text, 
            section.get('author', ''), 
            section.get('title', '')
        )
        
        # è¨­å•ã‚¿ã‚¤ãƒ—åˆ¥ã®çµ±è¨ˆ
        section_type_counts = {}
        for q in section_questions:
            q_type = q['type']
            section_type_counts[q_type] = section_type_counts.get(q_type, 0) + 1
        
        # ä¸»è¦ã‚¿ã‚¤ãƒ—ã‚’ç‰¹å®š
        if section_type_counts:
            main_types = sorted(section_type_counts.items(), key=lambda x: x[1], reverse=True)
            main_type_desc = ', '.join([f"{t}({c}å•)" for t, c in main_types])
        else:
            main_type_desc = "è¨­å•ãªã—"
        
        section_stats.append({
            'å¤§å•': f"å¤§å•{section_num}",
            'æ–‡ç« ã‚¸ãƒ£ãƒ³ãƒ«': genre,
            'ãƒ†ãƒ¼ãƒ': theme,
            'å‡ºå…¸_è‘—è€…': sanitize_for_excel(section.get('author', 'ä¸æ˜')),
            'å‡ºå…¸_ä½œå“': sanitize_for_excel(section.get('title', 'ä¸æ˜')),
            'è¨­å•æ•°': len(section_questions),
            'æ¨å®šæ–‡å­—æ•°': section_text_length,
            'å•é¡Œã‚¿ã‚¤ãƒ—æ§‹æˆ': sanitize_for_excel(main_type_desc)
        })
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ1ã‚·ãƒ¼ãƒˆã«ã¾ã¨ã‚ã‚‹ï¼‰
    # åŸºæœ¬æƒ…å ±
    basic_info = [
        ['åŸºæœ¬æƒ…å ±', '', '', '', '', '', '', ''],
        ['å­¦æ ¡å', 'æ¸‹æ¸‹ä¸­å­¦æ ¡', '', '', '', '', '', ''],
        ['å¹´åº¦', '2015', '', '', '', '', '', ''],
        ['åˆ†ææ—¥æ™‚', datetime.now().strftime('%Y/%m/%d %H:%M'), '', '', '', '', '', ''],
        ['ç·æ–‡å­—æ•°', f"{len(text):,}æ–‡å­—", '', '', '', '', '', ''],
        ['å¤§å•æ•°', len(sections), '', '', '', '', '', ''],
        ['ç·è¨­å•æ•°', len(all_questions), '', '', '', '', '', ''],
        ['', '', '', '', '', '', '', ''],  # ç©ºè¡Œ
    ]
    
    # å¤§å•åˆ¥è©³ç´°
    section_header = [['å¤§å•åˆ¥è©³ç´°', '', '', '', '', '', '', '']]
    section_data = []
    for stat in section_stats:
        section_data.append([
            stat['å¤§å•'],
            stat['æ–‡ç« ã‚¸ãƒ£ãƒ³ãƒ«'],
            stat['ãƒ†ãƒ¼ãƒ'],
            stat['å‡ºå…¸_è‘—è€…'],
            stat['å‡ºå…¸_ä½œå“'],
            f"{stat['è¨­å•æ•°']}å•",
            f"{stat['æ¨å®šæ–‡å­—æ•°']:,}æ–‡å­—",
            stat['å•é¡Œã‚¿ã‚¤ãƒ—æ§‹æˆ']
        ])
    
    # è¨­å•ã‚¿ã‚¤ãƒ—é›†è¨ˆ
    type_header = [['', '', '', '', '', '', '', ''], ['è¨­å•ã‚¿ã‚¤ãƒ—é›†è¨ˆ', '', '', '', '', '', '', '']]
    type_data = []
    for q_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(all_questions)) * 100
        type_data.append([q_type, f"{count}å•", f"{percentage:.1f}%", '', '', '', '', ''])
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ  
    all_data = basic_info + section_header + section_data + type_header + type_data
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    df = pd.DataFrame(all_data, columns=['é …ç›®', 'å€¤1', 'å€¤2', 'å€¤3', 'å€¤4', 'å€¤5', 'å€¤6', 'å€¤7'])
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å½¢å¼ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä½œæˆ
    create_database_excel(section_stats, all_questions, len(text))
    
    # é€šå¸¸ã®åˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='åˆ†æçµæœ', index=False, header=False)
    
    print(f"\nâœ… åˆ†æçµæœã‚’ä¿å­˜: {output_file}")
    
    return {
        'sections': sections,
        'questions': all_questions,
        'sources': sources,
        'type_counts': type_counts
    }


def create_database_excel(section_stats, all_questions, total_chars):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å½¢å¼ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆ1ã‚·ãƒ¼ãƒˆ1æ ¡ã€1åˆ—1å¹´åº¦ï¼‰"""
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
    db_filename = "entrance_exam_database.xlsx"
    school_name = "æ¸‹æ¸‹ä¸­å­¦æ ¡"
    year = 2015  # æ•´æ•°ã«ä¿®æ­£
    
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    try:
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        existing_sheets = pd.ExcelFile(db_filename, engine='openpyxl').sheet_names
    except FileNotFoundError:
        existing_sheets = []
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    data_row = {
        'å¹´åº¦': year,
        'ç·è¨­å•æ•°': len(all_questions),
        'ç·æ–‡å­—æ•°': total_chars,
        'å¤§å•æ•°': len(section_stats)
    }
    
    # å„å¤§å•ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    for i, stat in enumerate(section_stats, 1):
        data_row[f'å¤§å•{i}_ã‚¸ãƒ£ãƒ³ãƒ«'] = stat['æ–‡ç« ã‚¸ãƒ£ãƒ³ãƒ«']
        data_row[f'å¤§å•{i}_ãƒ†ãƒ¼ãƒ'] = stat['ãƒ†ãƒ¼ãƒ']
        data_row[f'å¤§å•{i}_è‘—è€…'] = stat['å‡ºå…¸_è‘—è€…']
        data_row[f'å¤§å•{i}_ä½œå“'] = stat['å‡ºå…¸_ä½œå“']
        data_row[f'å¤§å•{i}_è¨­å•æ•°'] = stat['è¨­å•æ•°']
        data_row[f'å¤§å•{i}_æ–‡å­—æ•°'] = stat['æ¨å®šæ–‡å­—æ•°']
    
    # è¨­å•ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
    type_counts = {}
    for q in all_questions:
        q_type = q['type']
        type_counts[q_type] = type_counts.get(q_type, 0) + 1
    
    for q_type, count in type_counts.items():
        data_row[f'{q_type}_å•é¡Œæ•°'] = count
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    new_df = pd.DataFrame([data_row])
    
    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    # if_sheet_existsã‚’'replace'ã«è¨­å®šã—ã¦ã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ç½®ãæ›ãˆ
    with pd.ExcelWriter(db_filename, engine='openpyxl', mode='a' if existing_sheets else 'w', if_sheet_exists='replace') as writer:
        if school_name in existing_sheets:
            # æ—¢å­˜ã‚·ãƒ¼ãƒˆã«è¿½åŠ 
            existing_df = pd.read_excel(db_filename, sheet_name=school_name)
            # åŒã˜å¹´åº¦ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°æ›´æ–°ã€ãªã‘ã‚Œã°è¿½åŠ 
            # å¹´åº¦åˆ—ã‚’æ•´æ•°å‹ã«çµ±ä¸€
            existing_df['å¹´åº¦'] = pd.to_numeric(existing_df['å¹´åº¦'], errors='coerce')
            if year in existing_df['å¹´åº¦'].values:
                existing_df = existing_df[existing_df['å¹´åº¦'] != year]
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            # å¹´åº¦åˆ—ã‚’å†åº¦æ•´æ•°å‹ã«å¤‰æ›ã—ã¦ã‹ã‚‰ã‚½ãƒ¼ãƒˆ
            combined_df['å¹´åº¦'] = pd.to_numeric(combined_df['å¹´åº¦'], errors='coerce')
            combined_df = combined_df.sort_values('å¹´åº¦')
        else:
            combined_df = new_df
        
        combined_df.to_excel(writer, sheet_name=school_name, index=False)
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°: {db_filename} - {school_name}ã‚·ãƒ¼ãƒˆ")


def find_bunko_ocr_text():
    """bunkoOCRã®çµæœãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰æœ€æ–°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢"""
    
    results_dir = Path.home() / "Library/Mobile Documents/iCloud~info~lithium03~bunkoOCR/Documents/Results"
    
    # æœ€æ–°ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¢ã™
    if results_dir.exists():
        folders = [d for d in results_dir.iterdir() if d.is_dir()]
        if folders:
            latest_folder = max(folders, key=lambda x: x.stat().st_mtime)
            text_files = list(latest_folder.glob("text*.txt"))
            
            if text_files:
                # å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆ
                combined_text = []
                for txt_file in sorted(text_files):
                    with open(txt_file, 'r', encoding='utf-8') as f:
                        combined_text.append(f.read())
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                temp_file = "shibuya_2015_bunko_combined.txt"
                with open(temp_file, 'w', encoding='utf-8') as f:
                    f.write('\n\n'.join(combined_text))
                
                print(f"âœ… bunkoOCRã®çµæœã‚’çµåˆ: {temp_file}")
                return temp_file
    
    return None


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    existing_files = [
        "/Users/yoshiikatsuhiko/Desktop/01_ä»•äº‹ (Work)/ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®¶åº­æ•™å¸«è³‡æ–™/éå»å•/æ¸‹æ¸‹/15æ¸‹æ¸‹.txt",
        "15æ¸‹æ¸‹_bunko.txt",
        "shibuya_2015_bunko.txt"
    ]
    
    text_file = None
    for file_path in existing_files:
        if Path(file_path).exists():
            text_file = file_path
            print(f"âœ… æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {text_file}")
            break
    
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯bunkoOCRã®çµæœã‚’æ¤œç´¢
    if not text_file:
        print("æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("bunkoOCRã®çµæœãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢ä¸­...")
        text_file = find_bunko_ocr_text()
    
    if text_file:
        # åˆ†æã‚’å®Ÿè¡Œ
        result = analyze_shibuya_2015_from_bunko_text(text_file)
        
        # ç›®æ¨™ã¨ã®æ¯”è¼ƒ
        print(f"\n{'='*60}")
        print("ã€ç²¾åº¦è©•ä¾¡ã€‘")
        print(f"æ¤œå‡ºã•ã‚ŒãŸè¨­å•æ•°: {len(result['questions'])}å•")
        print(f"ç›®æ¨™è¨­å•æ•°: 10-12å•ï¼ˆä¸€èˆ¬çš„ãªæ¸‹æ¸‹ã®å•é¡Œæ•°ï¼‰")
        
        if len(result['questions']) >= 10:
            print("âœ… é«˜ç²¾åº¦ã§ã®æ¤œå‡ºã«æˆåŠŸã—ã¾ã—ãŸï¼")
        else:
            print("âš ï¸ ä¸€éƒ¨ã®è¨­å•ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            print("bunkoOCRã§æœ€æ–°ã®å‡¦ç†ã‚’è¡Œã„ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦ãã ã•ã„")
    else:
        print("\nâŒ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("\nã€æ¬¡ã®æ‰‹é †ã€‘")
        print("1. bunkoOCRã§15æ¸‹æ¸‹.pdfã‚’å‡¦ç†")
        print("2. å‡¦ç†å®Œäº†å¾Œã€ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜")
        print("3. ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å†å®Ÿè¡Œ")


if __name__ == "__main__":
    main()