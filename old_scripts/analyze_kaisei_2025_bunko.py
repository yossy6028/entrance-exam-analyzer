#!/usr/bin/env python3
"""
é–‹æˆä¸­å­¦æ ¡2025å¹´åº¦å…¥è©¦å•é¡Œåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
bunkoOCRã§å‡¦ç†ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
"""
import re
from pathlib import Path
import pandas as pd
from datetime import datetime
import subprocess
import time
import pyautogui
import pyperclip


def launch_bunko_ocr_for_kaisei():
    """bunkoOCRã‚’èµ·å‹•ã—ã¦é–‹æˆä¸­2025å¹´ã®PDFã‚’å‡¦ç†"""
    
    print(f"\n{'='*60}")
    print(f"bunkoOCRã‚’ä½¿ç”¨ã—ãŸé–‹æˆä¸­å­¦æ ¡2025å¹´åº¦åˆ†æ")
    print(f"{'='*60}")
    
    # PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    pdf_path = "/Users/yoshiikatsuhiko/Desktop/01_ä»•äº‹ (Work)/ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®¶åº­æ•™å¸«è³‡æ–™/éå»å•/é–‹æˆä¸­å­¦æ ¡/2025å¹´é–‹æˆä¸­å­¦æ ¡å•é¡Œ_å›½èª.pdf"
    
    # bunkoOCRã‚’èµ·å‹•
    print("\nğŸ“± bunkoOCRã‚’èµ·å‹•ä¸­...")
    subprocess.Popen(['open', '-a', 'bunkoOCR'])
    time.sleep(3)
    
    # bunkoOCRã®PDFãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã‚’ã‚¯ãƒªãƒƒã‚¯
    print("ğŸ“‚ PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ...")
    pyautogui.click(x=689, y=475)  # PDFãƒ•ã‚¡ã‚¤ãƒ«é¸æŠãƒœã‚¿ãƒ³
    time.sleep(2)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã§PDFã‚’é¸æŠ
    pyperclip.copy(pdf_path)
    pyautogui.hotkey('cmd', 'shift', 'g')  # ãƒ‘ã‚¹å…¥åŠ›ãƒ€ã‚¤ã‚¢ãƒ­ã‚°
    time.sleep(0.5)
    pyautogui.hotkey('cmd', 'v')  # ãƒ‘ã‚¹ã‚’ãƒšãƒ¼ã‚¹ãƒˆ
    time.sleep(0.5)
    pyautogui.press('return')
    time.sleep(1)
    pyautogui.press('return')  # ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
    time.sleep(2)
    
    # OCRå®Ÿè¡Œ
    print("ğŸ” OCRå‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
    pyautogui.click(x=689, y=680)  # å®Ÿè¡Œãƒœã‚¿ãƒ³
    
    print("\nâ³ OCRå‡¦ç†ãŒå®Œäº†ã™ã‚‹ã¾ã§ãŠå¾…ã¡ãã ã•ã„ï¼ˆç´„2-3åˆ†ï¼‰")
    print("   å®Œäº†ã—ãŸã‚‰Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„...")
    input()
    
    # çµæœã®ä¿å­˜å ´æ‰€ã‚’ç¢ºèª
    results_dir = Path.home() / "Library/Mobile Documents/iCloud~info~lithium03~bunkoOCR/Documents/Results"
    if results_dir.exists():
        # æœ€æ–°ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—
        folders = [d for d in results_dir.iterdir() if d.is_dir()]
        if folders:
            latest_folder = max(folders, key=lambda x: x.stat().st_mtime)
            print(f"\nâœ… OCRçµæœãƒ•ã‚©ãƒ«ãƒ€: {latest_folder.name}")
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆ
            text_files = list(latest_folder.glob("text*.txt"))
            if text_files:
                combined_text = []
                for txt_file in sorted(text_files):
                    with open(txt_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        combined_text.append(f"=== {txt_file.name} ===\n{content}")
                
                # çµåˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
                output_file = "é–‹æˆ2025_bunko.txt"
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write('\n\n'.join(combined_text))
                
                print(f"âœ… çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜: {output_file}")
                return output_file
    
    return None


def analyze_kaisei_2025_from_bunko_text(text_file_path: str):
    """bunkoOCRã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é–‹æˆä¸­2025å¹´åº¦ã‚’åˆ†æ"""
    
    print(f"\n{'='*60}")
    print(f"é–‹æˆä¸­å­¦æ ¡2025å¹´åº¦ å…¥è©¦å•é¡Œåˆ†æ")
    print(f"{'='*60}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(text_file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    print(f"\nğŸ“„ èª­ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆ: {len(text)}æ–‡å­—")
    
    # å¤§å•æ§‹é€ ã®æ¤œå‡º
    sections = []
    
    # å¤§å•ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé–‹æˆä¸­å­¦æ ¡ç”¨ï¼‰
    section_patterns = [
        # ã€Œä¸€ã€æ¬¡ã®æ–‡ç« ã‚’èª­ã‚“ã§ã€ãƒ‘ã‚¿ãƒ¼ãƒ³
        (r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])ã€æ¬¡ã®æ–‡ç« ã‚’èª­ã‚“ã§', 'main_section_comma'),
        # ã€Œä¸€ æ¬¡ã®æ–‡ç« ã‚’èª­ã‚“ã§ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰
        (r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])\s+æ¬¡ã®æ–‡ç« ã‚’èª­ã‚“ã§', 'main_section_space'),
        # ã€Œç¬¬ä¸€å•ã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        (r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)å•', 'dai_mon'),
    ]
    
    # ã™ã¹ã¦ã®ãƒãƒƒãƒã‚’åé›†
    all_matches = []
    for pattern, p_type in section_patterns:
        for match in re.finditer(pattern, text, re.MULTILINE):
            all_matches.append({
                'start': match.start(),
                'end': match.end(),
                'number': match.group(1),
                'type': p_type,
                'full_match': match.group(0)
            })
    
    # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
    all_matches.sort(key=lambda x: x['start'])
    
    # å¤§å•ã‚’æ§‹ç¯‰
    for i, match in enumerate(all_matches):
        section_num = len(sections) + 1
        
        # å¤§å•ã®çµ‚äº†ä½ç½®ã‚’æ±ºå®š
        if i < len(all_matches) - 1:
            end_pos = all_matches[i + 1]['start']
        else:
            end_pos = len(text)
        
        sections.append({
            'number': section_num,
            'start_pos': match['start'],
            'end_pos': end_pos,
            'text': text[match['start']:end_pos],
            'type': 'æ–‡ç« èª­è§£'
        })
    
    # å¤§å•ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ä½“ã‚’1ã¤ã®å¤§å•ã¨ã—ã¦æ‰±ã†
    if not sections:
        sections.append({
            'number': 1,
            'start_pos': 0,
            'end_pos': len(text),
            'text': text,
            'type': 'æ–‡ç« èª­è§£'
        })
    
    print(f"\næ¤œå‡ºã•ã‚ŒãŸå¤§å•æ•°: {len(sections)}")
    
    # è¨­å•ã®æ¤œå‡º
    all_questions = []
    
    # è¨­å•ãƒ‘ã‚¿ãƒ¼ãƒ³
    question_patterns = [
        # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå•ä¸€ã€å•äºŒãªã©ï¼‰
        (r'å•([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)', 'kanji_num'),
        # OCRèª¤èªè­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé–“ãªã©ï¼‰
        (r'é–“([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)', 'kanji_num_ocr'),
        # å•1ã€å•2ãªã©ã®æ•°å­—ãƒ‘ã‚¿ãƒ¼ãƒ³
        (r'å•([1-9])', 'hankaku_num'),
    ]
    
    print(f"\nã€è¨­å•æ¤œå‡ºã€‘")
    
    for pattern, pattern_type in question_patterns:
        for match in re.finditer(pattern, text):
            # è¨­å•ã®å‘¨è¾ºãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            start = max(0, match.start() - 100)
            end = min(len(text), match.end() + 200)
            context = text[start:end].replace('\n', ' ')
            
            # è¨­å•ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            if 'æ¼¢å­—' in context or 'ã‚«ã‚¿ã‚«ãƒŠã‚’æ¼¢å­—ã«' in context:
                q_type = 'æ¼¢å­—ãƒ»èªå¥'
            elif 'è¨˜å·ã§ç­”ãˆãªã•ã„' in context or 'é¸ã³' in context:
                q_type = 'é¸æŠ'
            elif 'èª¬æ˜ã—ãªã•ã„' in context or 'ç†ç”±' in context:
                q_type = 'è¨˜è¿°'
            elif 'æŠœãå‡º' in context:
                q_type = 'æŠœãå‡ºã—'
            else:
                q_type = 'è¨˜è¿°'
            
            # ã©ã®å¤§å•ã«å±ã™ã‚‹ã‹åˆ¤å®š
            section_num = 1
            for section in sections:
                if section['start_pos'] <= match.start() < section['end_pos']:
                    section_num = section['number']
                    break
            
            question_info = {
                'section': section_num,
                'question': match.group(0),
                'type': q_type,
                'position': match.start(),
                'context': context[:150] + '...' if len(context) > 150 else context
            }
            
            # é‡è¤‡ã‚’é¿ã‘ã‚‹
            is_duplicate = False
            for existing_q in all_questions:
                if abs(existing_q['position'] - question_info['position']) < 50:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                all_questions.append(question_info)
                print(f"  {match.group(0)} - {q_type} (å¤§å•{section_num})")
    
    # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
    all_questions.sort(key=lambda x: x['position'])
    
    print(f"\nğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
    print(f"ç·è¨­å•æ•°: {len(all_questions)}")
    
    # è¨­å•ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
    type_counts = {}
    for q in all_questions:
        q_type = q['type']
        type_counts[q_type] = type_counts.get(q_type, 0) + 1
    
    print(f"\nè¨­å•ã‚¿ã‚¤ãƒ—åˆ¥å†…è¨³:")
    for q_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(all_questions)) * 100
        print(f"  {q_type}: {count}å• ({percentage:.1f}%)")
    
    # å‡ºå…¸æƒ…å ±ã®æŠ½å‡º
    print(f"\nğŸ“š å‡ºå…¸æƒ…å ±ã®æ¤œç´¢")
    
    # å‡ºå…¸ãƒ‘ã‚¿ãƒ¼ãƒ³
    source_patterns = [
        r'ï¼ˆ([^ï¼‰]{1,50}ã€[^ã€]{1,100}ã€[^ï¼‰]{0,20})ï¼‰',  # æ—¥æœ¬èªæ‹¬å¼§
        r'ï¼ˆ([^ï¼‰]{1,50}ã€Œ[^ã€]{1,100}ã€[^ï¼‰]{0,20})ï¼‰',
        r'\(([^)]{1,50}ã€[^ã€]{1,100}ã€[^)]{0,20})\)',  # åŠè§’æ‹¬å¼§
    ]
    
    sources = []
    for pattern in source_patterns:
        for match in re.finditer(pattern, text):
            source_text = match.group(1)
            if len(source_text) > 150:
                continue
            
            # è‘—è€…åã¨ä½œå“åã‚’æŠ½å‡º
            if 'ã€' in source_text:
                parts = source_text.split('ã€')
                if len(parts) >= 2:
                    author = parts[0].strip()
                    title = parts[1].split('ã€')[0].strip()
                    if 1 <= len(author) <= 20 and 1 <= len(title) <= 50:
                        sources.append({
                            'author': author,
                            'title': title,
                            'full': source_text,
                            'position': match.start()
                        })
    
    # å„å¤§å•ã®å‡ºå…¸ã‚’ç‰¹å®š
    for section in sections:
        section_sources = [s for s in sources if section['start_pos'] <= s['position'] < section['end_pos']]
        if section_sources:
            # æœ€ã‚‚è¿‘ã„å‡ºå…¸ã‚’é¸æŠ
            closest_source = min(section_sources, key=lambda x: abs(x['position'] - section['start_pos']))
            section['author'] = closest_source['author']
            section['title'] = closest_source['title']
            print(f"\nå¤§å•{section['number']}ã®å‡ºå…¸:")
            print(f"  è‘—è€…: {closest_source['author']}")
            print(f"  ä½œå“: ã€{closest_source['title']}ã€")
        else:
            section['author'] = 'ä¸æ˜'
            section['title'] = 'ä¸æ˜'
    
    # Excelç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    def sanitize_for_excel(text):
        """Excelä¿å­˜ç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""
        if not text:
            return ""
        
        # ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
        problematic_chars = {
            '\ufff9', '\ufffa', '\ufffb', '\ufeff',
            '\u200b', '\u200c', '\u200d',
        }
        cleaned = ''.join(char for char in text if char not in problematic_chars)
        
        # å±±æ‹¬å¼§ã€ˆã€‰ã‚’é€šå¸¸ã®æ‹¬å¼§ã«ç½®æ›
        cleaned = cleaned.replace('ã€ˆ', 'ï¼œ').replace('ã€‰', 'ï¼')
        
        # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
        sanitized = ''.join(char for char in cleaned if ord(char) >= 32 or char in '\n\r\t')
        
        # å…ˆé ­ã®ç‰¹æ®Šæ‹¬å¼§ã‚’ä¿®æ­£
        if sanitized.startswith('ï¼œ') or sanitized.startswith('ã€ˆ'):
            sanitized = 'ã€Œ' + sanitized[1:]
        if sanitized.endswith('ï¼') or sanitized.endswith('ã€‰'):
            sanitized = sanitized[:-1] + 'ã€'
        
        return sanitized[:500]
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å½¢å¼ã§ä¿å­˜
    create_database_excel(sections, all_questions, len(text), sanitize_for_excel)
    
    # é€šå¸¸ã®åˆ†æçµæœã‚‚ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f"é–‹æˆä¸­å­¦æ ¡_2025_bunkoåˆ†æçµæœ_{timestamp}.xlsx"
    
    # åˆ†æçµæœã‚’Excelã«ä¿å­˜
    save_analysis_result(output_file, sections, all_questions, type_counts, len(text), sanitize_for_excel)
    
    print(f"\nâœ… åˆ†æçµæœã‚’ä¿å­˜: {output_file}")
    
    return {
        'sections': sections,
        'questions': all_questions,
        'type_counts': type_counts
    }


def create_database_excel(sections, all_questions, total_chars, sanitize_func):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å½¢å¼ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆãƒ»æ›´æ–°"""
    
    db_filename = "entrance_exam_database.xlsx"
    school_name = "é–‹æˆä¸­å­¦æ ¡"
    year = 2025
    
    try:
        existing_sheets = pd.ExcelFile(db_filename, engine='openpyxl').sheet_names
    except FileNotFoundError:
        existing_sheets = []
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’æº–å‚™
    data_row = {
        'å¹´åº¦': year,
        'ç·è¨­å•æ•°': len(all_questions),
        'ç·æ–‡å­—æ•°': total_chars,
        'å¤§å•æ•°': len(sections)
    }
    
    # å„å¤§å•ã®ãƒ‡ãƒ¼ã‚¿
    for i, section in enumerate(sections, 1):
        # æ–‡ç« ã‚¸ãƒ£ãƒ³ãƒ«ã¨ãƒ†ãƒ¼ãƒã‚’åˆ¤å®š
        section_text = section['text'][:1000]
        
        # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®š
        if any(word in section_text for word in ['å°èª¬', 'ç‰©èª', 'ã€Œ', 'ã€', 'ã¨è¨€ã£ãŸ']):
            genre = 'å°èª¬ãƒ»ç‰©èª'
        elif any(word in section_text for word in ['è©•è«–', 'è«–èª¬', 'ã«ã¤ã„ã¦']):
            genre = 'è©•è«–ãƒ»è«–èª¬'
        elif any(word in section_text for word in ['éšç­†', 'ã‚¨ãƒƒã‚»ã‚¤', 'ç§ã¯']):
            genre = 'éšç­†ãƒ»ã‚¨ãƒƒã‚»ã‚¤'
        else:
            genre = 'è©•è«–ãƒ»è«–èª¬'
        
        # ãƒ†ãƒ¼ãƒåˆ¤å®š
        if any(word in section_text for word in ['å®¶æ—', 'å‹æƒ…', 'æˆé•·']):
            theme = 'äººé–“é–¢ä¿‚ãƒ»æˆé•·'
        elif any(word in section_text for word in ['è‡ªç„¶', 'ç’°å¢ƒ', 'ç”Ÿç‰©']):
            theme = 'è‡ªç„¶ãƒ»ç’°å¢ƒ'
        elif any(word in section_text for word in ['ç¤¾ä¼š', 'æ–‡åŒ–', 'æ­´å²']):
            theme = 'ç¤¾ä¼šãƒ»æ–‡åŒ–'
        else:
            theme = 'ä¸€èˆ¬'
        
        section_questions = [q for q in all_questions if q['section'] == section['number']]
        
        data_row[f'å¤§å•{i}_ã‚¸ãƒ£ãƒ³ãƒ«'] = genre
        data_row[f'å¤§å•{i}_ãƒ†ãƒ¼ãƒ'] = theme
        data_row[f'å¤§å•{i}_è‘—è€…'] = sanitize_func(section.get('author', 'ä¸æ˜'))
        data_row[f'å¤§å•{i}_ä½œå“'] = sanitize_func(section.get('title', 'ä¸æ˜'))
        data_row[f'å¤§å•{i}_è¨­å•æ•°'] = len(section_questions)
        data_row[f'å¤§å•{i}_æ–‡å­—æ•°'] = len(section['text'])
    
    # è¨­å•ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
    type_counts = {}
    for q in all_questions:
        q_type = q['type']
        type_counts[q_type] = type_counts.get(q_type, 0) + 1
    
    for q_type, count in type_counts.items():
        data_row[f'{q_type}_å•é¡Œæ•°'] = count
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    new_df = pd.DataFrame([data_row])
    
    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    with pd.ExcelWriter(db_filename, engine='openpyxl', mode='a' if existing_sheets else 'w', if_sheet_exists='replace') as writer:
        if school_name in existing_sheets:
            existing_df = pd.read_excel(db_filename, sheet_name=school_name)
            existing_df['å¹´åº¦'] = pd.to_numeric(existing_df['å¹´åº¦'], errors='coerce')
            if year in existing_df['å¹´åº¦'].values:
                existing_df = existing_df[existing_df['å¹´åº¦'] != year]
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            combined_df['å¹´åº¦'] = pd.to_numeric(combined_df['å¹´åº¦'], errors='coerce')
            combined_df = combined_df.sort_values('å¹´åº¦')
        else:
            combined_df = new_df
        
        combined_df.to_excel(writer, sheet_name=school_name, index=False)
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°: {db_filename} - {school_name}ã‚·ãƒ¼ãƒˆ")


def save_analysis_result(output_file, sections, all_questions, type_counts, total_chars, sanitize_func):
    """åˆ†æçµæœã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    
    # åŸºæœ¬æƒ…å ±
    basic_info = [
        ['åŸºæœ¬æƒ…å ±', '', '', '', '', '', '', ''],
        ['å­¦æ ¡å', 'é–‹æˆä¸­å­¦æ ¡', '', '', '', '', '', ''],
        ['å¹´åº¦', '2025', '', '', '', '', '', ''],
        ['åˆ†ææ—¥æ™‚', datetime.now().strftime('%Y/%m/%d %H:%M'), '', '', '', '', '', ''],
        ['ç·æ–‡å­—æ•°', f"{total_chars:,}æ–‡å­—", '', '', '', '', '', ''],
        ['å¤§å•æ•°', len(sections), '', '', '', '', '', ''],
        ['ç·è¨­å•æ•°', len(all_questions), '', '', '', '', '', ''],
        ['', '', '', '', '', '', '', ''],
    ]
    
    # å¤§å•åˆ¥è©³ç´°
    section_header = [['å¤§å•åˆ¥è©³ç´°', '', '', '', '', '', '', '']]
    section_data = []
    
    for section in sections:
        section_questions = [q for q in all_questions if q['section'] == section['number']]
        
        # è¨­å•ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
        section_type_counts = {}
        for q in section_questions:
            q_type = q['type']
            section_type_counts[q_type] = section_type_counts.get(q_type, 0) + 1
        
        type_desc = ', '.join([f"{t}({c}å•)" for t, c in section_type_counts.items()])
        
        section_data.append([
            f"å¤§å•{section['number']}",
            'æ–‡ç« èª­è§£',
            '',
            sanitize_func(section.get('author', 'ä¸æ˜')),
            sanitize_func(section.get('title', 'ä¸æ˜')),
            f"{len(section_questions)}å•",
            f"{len(section['text']):,}æ–‡å­—",
            type_desc
        ])
    
    # è¨­å•ã‚¿ã‚¤ãƒ—é›†è¨ˆ
    type_header = [['', '', '', '', '', '', '', ''], ['è¨­å•ã‚¿ã‚¤ãƒ—é›†è¨ˆ', '', '', '', '', '', '', '']]
    type_data = []
    for q_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(all_questions)) * 100
        type_data.append([q_type, f"{count}å•", f"{percentage:.1f}%", '', '', '', '', ''])
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    all_data = basic_info + section_header + section_data + type_header + type_data
    df = pd.DataFrame(all_data)
    
    # Excelã«ä¿å­˜
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='åˆ†æçµæœ', index=False, header=False)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    
    # æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    if Path("é–‹æˆ2025_bunko.txt").exists():
        print("âœ… æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: é–‹æˆ2025_bunko.txt")
        text_file = "é–‹æˆ2025_bunko.txt"
    else:
        # bunkoOCRã§å‡¦ç†
        text_file = launch_bunko_ocr_for_kaisei()
        if not text_file:
            print("âŒ bunkoOCRã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
    
    # åˆ†æå®Ÿè¡Œ
    result = analyze_kaisei_2025_from_bunko_text(text_file)
    
    print(f"\n{'='*60}")
    print(f"ã€ç²¾åº¦è©•ä¾¡ã€‘")
    print(f"æ¤œå‡ºã•ã‚ŒãŸè¨­å•æ•°: {len(result['questions'])}å•")
    print(f"ç›®æ¨™è¨­å•æ•°: 10-15å•ï¼ˆä¸€èˆ¬çš„ãªé–‹æˆã®å•é¡Œæ•°ï¼‰")
    
    if 10 <= len(result['questions']) <= 15:
        print(f"âœ… é«˜ç²¾åº¦ã§ã®æ¤œå‡ºã«æˆåŠŸã—ã¾ã—ãŸï¼")
    else:
        print(f"âš ï¸  è¨­å•æ•°ãŒæƒ³å®šç¯„å›²å¤–ã§ã™ã€‚OCRç²¾åº¦ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()